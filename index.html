<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="style.css">
    <title>Advanced Interactive Prototyping FA20 - Anthony Palileo</title>
</head>

<body>
    <div class="container">

        <div class="header">
            <div class="logo">
                <img src="images/AP Logo REV 06.png" style="width:60px">
            </div>
            <div class="title">
                <h3>Anthony Palileo</h3>
                <h4>Advanced Interactive Prototyping</h4>
            </div>
        </div>

        <div class="content">
            <p>My collection of posts for Fall 2020 at ArtCenter College of Design in Pasadena, California </p>
        </div>

        <banner class="accordion"><b>Project 4: Interconnected Systems</b></banner>
        <div class="panel">
            <br>
            <p>The final assignment encourages us to bring together all of our lessons into a single concept, whether
                that is expanding on a previous project or developing an entirely new one. I opted to for the latter and
                explored a new system using new inputs and a different types of visualization.</p>
            <br>
            <h3>Concept</h3>
            <br>
            <p>
                The initial concept was to measure biometric data in the forms of heart rate (BPM) and body temperature
                (F) to measure the state of the user and provide a scale of mental states for them to select from (e.g.
                calm, happy, stressed, etc.) to see if there is a correlation between the individual perception and the
                actual physical state.</p>
            <p>
                In looking deeper into this idea, I was given the suggestion to look into the Quantified Self, in which
                a wide array of metrics were measured of users and visualized in numerous ways. I began to think upon
                ways to further develop this connection between mental and physical state. My reiteration involved
                taking the same biometric data and implementing it against some physical input that would encourage the
                user to focus on a singular task with the goal of creating a calm, focused state.</p>
            <br>
            <h3>Tools & Methods</h3>
            <br>
            <p>For gathering biometrics, I used a pulse sensor and a 1-wire temperature sensor. For user input I
                incorporated a force resistor. I programmed a Particle Argon board to send the data via serial to a
                Processing sketch. I soon discovered that the existing programs for the pulse sensor were geared for the
                Particle Proton and Core boards, so some work was required to get it working with the Particle Argon
                board. Some online referencing and stripping of the original code got it up an running.</p>
            <br>
            <img src="images/DSC01304.JPG" class="center" style="width:60%">
            <br>
            <img src="images/DSC01302.JPG" class="center" style="width:60%">
            <br>
            <h3>System Diagram</h3>
            <br>
            <p>The plan is simple: three inputs from the user affect points on the canvas that have color, movement, separation, and cohesion.</p>
            <br>
            <img src="images/FinalDiagram.jpg" class="center" style="width:60%">
            <br>
            <h3>Visualization</h3>
            <br>
            <p>My first inspiration came by way of Craig Reynold's Boids program, a simulation of flocking. Playing with
                the characteristics of cohesion and separation, I intended on challenging the user to create a balance
                of cohesion to the separation created by their biometrics, resulting in a more uniform flocking of the
                points on the interface.</p>
            <br>
            <img src="images/Boids.gif" class="center" style="width:60%">
            <br>
            <p>Initial response revealed that this visualization does not provide a calming or focus effect for some
                users, and some are even further stressed by the movement of so many points. I was given several ideas,
                one of which was looking into metaballs. After a few A-B tests it proved that this would be a much more
                effective visualization if treated correctly.</p>
            <br>
            <img src="images/Metaballs.gif" class="center" style="width:60%">
            <br>
            <p>The inherent nature of the design of the metaballs program is that the movement is completely random,
                with no direct relationship between the points and their behavior. My resolution was to hybridize the
                two programs I had been exploring. The result is what I like to refer to as "MetaBoids". They are
                essentially the flocking Boids with the pixel relationships of metaballs, giving me the visualization of
                the metalballs with the cohesion and separation variables of the Boids.</p>
            <p>
                Once the Particle serial data was plugged into the sketch, I was able to manipulate the color values,
                rate of movement, and the cohesion of the points and dial them in to provide a visual representation for
                users to focus their attention on.</p>
            <br>
            <img src="images/Metaboids.gif" class="center" style="width:60%">
            <br>
            <h3>Form Factor</h3>
            <br>
            <p>The existing form factor consists of a pulse sensor strapped to my left pointer finger, a temperature sensor tucked into my waistband against my skin, and a force sensor taped to a lacrosse ball. In a real world application, the pulse sensor would be built into a pad on which a user would rest their hand, a temperature gun situated somewhere atop a display monitor, and a joystick like fixture with the force sensor built into it.</p>
            <br>
            <img src="images/DSC01324.JPG" class="center" style="width:60%">
            <br>
            <h3>Finishing Touches</h3>
            <br>
            <p>This involved fine tuning details such as visual scale, input thresholds, and color application. This five minute time lapse shows the color change as the sensor comes to my body temperature before I engage with the force sensor. I found that going from a hotter purple to a softer blue-green provided a more calming overall visual. In a few tests I discovered focusing on the uniform object effectively lowered my heart rate.</p>
            <br>
            <img src="images/Focus.gif" class="center" style="width:60%">
            <p>Five minute time lapse.</p>
            <br>
            <h3>Future Consideration</h3>
            <br>
            <p>In a situation where in-person interaction is once again deemed safe, I would love to user test the system to gather more insight on the effects of certain color applications and visual treatments.</p>
            <p>I was pleasantly surprised at the results I managed to gather from my own trials and would like to test hypotheses regarding this single task oriented interaction.</p>
            <br>
            <h3>Future Iteration</h3>
            <br>
            <p>Looking forward, I would first be interested in exploring other means of biometric data gathering. I would want the system to be as minimally invasive as possible.</p>
            <p>Then I would consider the incorporation of sound, either by way of white noise or relaxing soundtrack, added control of mediating heart rate, as well as the possibility of adding metrics of brain activity into the concept.</p>
        </div>

        <banner class="accordion"><b>Project 3: Published Sensor System</b></banner>
        <div class="panel">
            <br>
            <p>For the third project, we are publishing our Particle board results to an API. We are able to monitor it
                through ThingSpeak IoT with the ultimate goal of using p5.js to read the data and "do something visual"
                with it.</p>
            <p>I chose to measure the interactions I have with my keyboard and mouse. I began by measuring the outputs
                created by attaching a piezo to my keyboard and a force resistor to my mouse's left banner. The issue I
                was facing was that the force resistor peaked way too easily. To adjust for this, I simply replaced that
                with another piezo, attached on the top of my mouse to capture vibrations from clicking.</p>
            <br>
            <img src="images/xDSC01299.jpg" class="center" style="width:60%">
            <br>
            <p>Getting a proper readout from each piezo was another challenge. As I console.logged the incoming data, I
                was seeing that the 1000ms publishing rate was spotty at best, meaning I would have to be clicking my
                mouse or typing on my keyboard at the exact moment the data was published. I banged my head for a while
                staring at the wrong end of the data, and trying to write the sketch to work it out. I ended up playing
                with the console IDE for the Particle board to do some math before publishing. Rather than using
                delay(), I leaned on millis() to do some work for me in the loop(). I would total up all of the readings
                over one second (1000ms), average them, and send that data as my reference. Meaning any second where
                there was some sort of activity in either sensor should increase the average above a certain threshold.
                Eureka! I was getting somewhere.</p>
            <br>
            <br>
            <img src="images/2020-10-31.png" class="center" style="width:60%">
            <br>
            <img src="images/2020-11-01.png" class="center" style="width:60%">
            <br>
            <img src="images/2020-11-02 Motion.png" class="center" style="width:60%">
            <br>
            <p>I admittedly decided against a traditional time-based "this is keyboard/this is mouse" display. Instead,
                I wanted to "paint a picture" of my use over a given time. The first iterations of my sketch would
                populate the screen with random semi transparent dots, magenta for the mouse and cyan for the keyboard.
                To give them a bit more "life", I programmed them to wiggle around.</p>
            <br>
            <img src="images/SensorsWiggle.gif" class="center" style="width:60%">
            <br>
            <p>Going into my third iteration, I decided I wanted something a bit more dynamic than just circles on a
                canvas (which kind of just look like bubbles). I set out to create a randomized "brush" to represent
                these readings. This iteration introduced "particles" which would roam the canvas with zero opacity
                until a data point above a set threshold would assign them to show at 10%. In response to class
                feedback, I gave each data set a different shape in addition to the color. Keyboard activity is
                represented by cyan rectangles and mouse activity, magenta ellipses.</p>
            <p>I limited the canvas to 1080x1080 rather than setting to window height and width. This would give me more
                consistent final pieces to represent together since I don't always have the browser window set to a
                fixed dimension when accessing the index.</p>
            <p>A lot of time was spent mapping and remapping the scales of the particles. Too little and the overall
                compositions were a bit uneventful. Too large and they consumed the canvas almost immediately. The more
                intense the vibrations, the higher the readout, the larger the particle. Simple enough! Once I figured
                out my scaling, I set out to measure the interactions over long periods of time.</p>
            <br>
            <img src="images/2020-11-03b.png" class="center" style="width:40%">
            <br>
            <img src="images/2020-11-04.png" class="center" style="width:40%">
            <br>
            <img src="images/2020-11-06.png" class="center" style="width:40%">
            <br>
            <p>As shown, the compositions were unique and very telling. Like the previous bubbles sketches, there is a
                noticeable difference between the amount of keyboard and mouse activity on any given day, with some days
                more focused on the keyboard (like typing out this documentation) or days where there was far more mouse
                clicking (such as working on Motion Design 1 homework on After Effects). This was an entertaining study
                in how we can produce beautiful artwork by simply going about our business through the day.</p>
        </div>


        <banner class="accordion"><b>Project 2: Interactive Natural System</b></banner>
        <div class="panel">
            <br>
            <p>The second project introduces a Particle Argon board taking inputs from potentiometers and a tactile
                button and sending strings of data to be used in the Natural System. To keep things easier to manage, I
                opted to forgo the whole tadpole eats lettuce and focused on the frog eating the flies.</p>
            <p>One major addition to this is the rewriting of code to incorporate forces that repel the flies from the
                frog. Initially I began by writing in PVectors in place of variables like posX and posY. A few
                iterations in I found it best to just start from scratch and write the code around these parameters and
                then bring the frog and fly images in.</p>
            <p>The controller is built into a Sparkfun box with the tactile button placed just above the right
                potentiometer knob. The left knob controls the X axis movement and the right knob controls Y axis
                movement, much like an Etch-a-Sketch. However, these controls are mapped to full negative or full
                positive velocity, so unlike an Etch-a-Sketch "cursor", the frog continues in the directions according
                to the knob positions. The tactile button serves to eat the flies, but only if they are within the set
                "striking range"</p>
            <br>
            <p>The controller:</p>
            <img src="images/ParticleController1.jpg" class="center" style="width:60%">
            <br>
            <img src="images/ParticleController2.jpg" class="center" style="width:60%">
            <br>
            <img src="images/ParticleController3.jpg" class="center" style="width:60%">
            <br>
            <p>For your viewing consideration, a quick demo video:
                <iframe src="https://player.vimeo.com/video/470463256" class="center" width="640" height="480"
                    frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></p>
            <p>For future consideration, I would probably add a left tactile button, just for left handed or even
                ambidextrous engagement.</p>
        </div>

        <banner class="accordion"><b>Reading Response: <i>What do Prototypes Prototype?</i> Houde, Stephanie and Charles
                Hill</b></banner>
        <div class="panel">
            <br>
            <p><i>"Everyone has a different expectation of what a prototype is. Industrial designers call a molded foam
                    model a prototype. Interaction designers refer to a simulation of on-screen appearance and behavior
                    as a prototype. Programmers call a test program a prototype. A user studies expert may call a
                    storyboard, which shows a scenario of something being used, a prototype."</i></p>
            <p>This is a very accurate statement, and is even relevant to this class. I came in under a preconceived
                notion that the type of interactive prototyping I would be designing would be just like that of previous
                "interactive prototyping" courses I had taken. I was creating simulations of web pages and mobile apps
                prior, and each of those courses had different levels of prototypes within them. Some were paper, others
                were "low-fidelity," while others were basically unpublished working products. Now I am on the cusp of
                creating a protytype that exists off a page or screen.</p>
            <p>The term "prototype" has been thrown around so much it requires context to understand the content. Using
                Hollywood as an example, Tony Stark of Marvel's <i>Iron Man</i> movies meets with his military colleague
                James Rhodes to show him a "prototype" of his latest exosuit, but he flies in with it, fully functional
                and ready to equip with the latest military-grade installations. The authors of the book define
                <i>prototype</i> as <i>"any representation of a design idea, regardless the medium."</i> So the pressing
                question for the exosuit (which would become the War Machine) is "what then, is this prototyping, if it
                is not the finished suit?" My guess is: it was the paint job.</p>
            <br>
            <p><i>"Storyboards ... are considered to be effective design tools by many designers because they help focus
                    design discussion on the role of an artifact very early on. However, giving them status as
                    prototypes is not common because the medium is paper and thus seems very far from the medium of an
                    interactive computer system. We consider this storyboard to be a prototype because it makes a
                    concrete representation of a design idea and serves the purpose of asking and answering design
                    questions."</i></p>
            <p>I am definitely on board with this line of thinking. While it is true that paper leaves far more to the
                imagination for the user test participants, it is also the most maleable form of prototyping with the
                least amount of commitment. Even simple digital wireframes take more effort than pen and paper. On paper
                it is easy to cross things out, take notes right on the prototype and at the worst, crumple it up and
                start over. No harm done.I would even go as far as to suggest that participants in users tests may even
                be more open to adverse opinion as they are not plagued with the concerns of "hurting someone's
                feelings" with regard to their design. I have experienced great successes with paper prototyping and
                will continue to do it in future design.</p>
            <br>
            <p><i>"High quality appearance models are costly to build. There are two common reasons for investing in
                    one: to get a visceral response by making the design seem “real” to any audience (design team,
                    organization, and potential users); and to verify the intended look and feel of the artifact before
                    committing to production tooling. "</i></p>
            <p>I find myself on the fence regarding the idea of these types of models, and I would definitely err on the
                side of caution when considering this implementation. I'm certain that these are only created after
                other extensive prototyping sessions have been exhausted, reaching this state. However, in my
                experience, I have seen many a prototype come to this level prematurely only to be completley picked
                apart and inevitably discarded. I would suggest that this is very true for the automotive industry. Year
                after year, concept cars are rolled out on display at Geneva, Detroit, and other major auto shows, only
                to be set aside and forgotten. Many of these are painstakingly build to production level, some even
                equipped with a fully functional drivetrain.</p>
        </div>

        <banner class="accordion"><b>Project 1: Natural Systems</b></banner>
        <div class="panel">
            <br>
            <p>The assignment: ideate a natural system and design it in Processing. I chose to prototype a pond in
                which a frog grows up. It starts life as a tadpole eating lettuce and grows to a frog that eats
                flies. I've always been fascinated by amphibians, frogs in particular. They have an interesting life
                cycle and evolutionary history. My favorites are the poison dart frogs of Central and South America
                with their bright colors, small stature, and toxic protective system.</p>
            <p>This is my initial diagram:</p>
            <img src="NaturalSystemProject/Anthony's Scratch Board - Adv. Interactive Prototyping - Natural System Diagram v1.jpg"
                class="center" style="width: 60%">
            <p>After feedback and consideration, it served best to work without the constraint of the lily pad and
                to consider the tadpole and the frog as two versions of the same object. This is the resulting
                diagram:</p>
            <img src="NaturalSystemProject/Anthony's Scratch Board - Adv. Interactive Prototyping - Natural System Diagram v2.jpg"
                class="center" style="width: 60%">
            <p>With this planned out, I went through several iterations of coding, figuring things out one bit at a
                time. By version 6 I had a pretty good working prototype:</p>
            <img src="images/NaturalSystem.gif" class="center">
            <p>I tried to keep the translation of the system to code as literal as possible. Tadpoles eat lettuce
                that users can click onto the screen, and at a certain point, grow up to become frogs that eat the
                flies that randomly appear. I initially had two user interactions planned out, but accomplishing the
                first was as much as I could accomplish within the deadline.</p>
            <p>For future iteration, I plan on figuring out how to rotate the images relative to their direction of
                movement. I'd like to reintroduce the idea of the lily pads as constraints for the adult frog's
                movement, and I also considered having the flies compete with the tadpoles for the lettuce.</p>
        </div>

        <div class="footer">
            Copyright © 2020 Anthony Palileo. All Rights Reserved.
        </div>

    </div>


    <script>
        var acc = document.getElementsByClassName("accordion");
        var i;
        for (i = 0; i < acc.length; i++) {
            acc[i].addEventListener("click", function () {
                this.classList.toggle("active");
                var panel = this.nextElementSibling;
                if (panel.style.maxHeight) {
                    panel.style.maxHeight = null;
                } else {
                    panel.style.maxHeight = "60%";
                }
            });
        }
    </script>
</body>

</html>